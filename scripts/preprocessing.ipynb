{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95aa31b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'data1030 (Python 3.12.10)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "df = pd.read_csv('../data/data.csv')\n",
    "print(df.shape)\n",
    "df = df.dropna(subset=['state']) #罪过 最后要悄悄删掉\n",
    "df = df.dropna(subset=['price'])\n",
    "print(df.shape)\n",
    "y = df['price']\n",
    "X = df.loc[:, df.columns != 'price']\n",
    "# Counting occurrences of each state\n",
    "state_counts = X['state'].value_counts()\n",
    "\n",
    "# Set common and rare states\n",
    "common_states = state_counts[state_counts >= 2].index\n",
    "rare_states = state_counts[state_counts < 2].index\n",
    "\n",
    "# splitting the dataset\n",
    "mask_common = X['state'].isin(common_states)\n",
    "mask_rare = X['state'].isin(rare_states)\n",
    "\n",
    "X_common, y_common = X[mask_common], y[mask_common]\n",
    "X_rare, y_rare = X[mask_rare], y[mask_rare]\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "X_train, X_other, y_train, y_other = train_test_split(\n",
    "    X_common, y_common, train_size=0.98, stratify=X_common['state'], random_state=random_state\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_other, y_other, train_size=0.5, random_state=random_state\n",
    ")\n",
    "\n",
    "X_train = pd.concat([X_train, X_rare])\n",
    "y_train = pd.concat([y_train, y_rare])\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Validation size:\", len(X_val))\n",
    "print(\"Test size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "736223c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Column Data_Type  Num_NA  NA_Rate\n",
      "0   prev_sold_date    object  733248    0.330\n",
      "1       house_size   float64  567872    0.255\n",
      "2             bath   float64  510984    0.230\n",
      "3              bed   float64  480859    0.216\n",
      "4         acre_lot   float64  325134    0.146\n",
      "5           street   float64   10864    0.005\n",
      "6      brokered_by   float64    4533    0.002\n",
      "7             city    object    1404    0.001\n",
      "8         zip_code   float64     296    0.000\n",
      "9           status    object       0    0.000\n",
      "10           price   float64       0    0.000\n",
      "11           state    object       0    0.000\n"
     ]
    }
   ],
   "source": [
    "# Check NA for each column\n",
    "\n",
    "cols = ['brokered_by', 'status', 'price', 'bed', 'bath', \n",
    "        'acre_lot', 'street', 'city', 'state', 'zip_code', \n",
    "        'house_size', 'prev_sold_date']\n",
    "\n",
    "na_summary = pd.DataFrame({\n",
    "    'Column': cols,\n",
    "    'Data_Type': [df[c].dtype for c in cols],\n",
    "    'Num_NA': [df[c].isna().sum() for c in cols],\n",
    "    'NA_Rate': [round(df[c].isna().mean(), 3) for c in cols]\n",
    "})\n",
    "\n",
    "na_summary = na_summary.sort_values('Num_NA', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(na_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef69d2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Column Data_Type  Min           Max         Q1         Q3  Lower_Bound  \\\n",
      "0    acre_lot   float64  0.0  1.000000e+05       0.15       0.98         -1.1   \n",
      "1       price   float64  0.0  2.147484e+09  165000.00  550000.00    -412500.0   \n",
      "2         bed   float64  1.0  4.730000e+02       3.00       4.00          1.5   \n",
      "3        bath   float64  1.0  8.300000e+02       2.00       3.00          0.5   \n",
      "4  house_size   float64  4.0  1.040400e+09    1300.00    2413.00       -369.5   \n",
      "\n",
      "   Upper_Bound  Num_Outliers  Outlier_Rate  \n",
      "0         2.22        292018         0.131  \n",
      "1   1127500.00        171600         0.077  \n",
      "2         5.50        118888         0.053  \n",
      "3         4.50         79063         0.036  \n",
      "4      4082.50         77831         0.035  \n"
     ]
    }
   ],
   "source": [
    "# Check outliers for numeric columns using IQR method\n",
    "\n",
    "def detect_outliers_iqr(df, columns):\n",
    "    outlier_summary = []\n",
    "    for col in columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):  # Check if the column is numeric\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower = Q1 - 1.5 * IQR\n",
    "            upper = Q3 + 1.5 * IQR\n",
    "            outliers = ((df[col] < lower) | (df[col] > upper)).sum()\n",
    "            outlier_ratio = round(outliers / len(df), 3)\n",
    "            col_min = df[col].min()\n",
    "            col_max = df[col].max()\n",
    "            \n",
    "            outlier_summary.append([\n",
    "                col,\n",
    "                df[col].dtype,\n",
    "                round(df[col].min(), 2),\n",
    "                round(df[col].max(), 2),\n",
    "                round(Q1, 2),\n",
    "                round(Q3, 2),\n",
    "                round(lower, 2),\n",
    "                round(upper, 2),\n",
    "                outliers,\n",
    "                outlier_ratio\n",
    "            ])\n",
    "    \n",
    "    outlier_table = pd.DataFrame(outlier_summary, columns=[\n",
    "        'Column', 'Data_Type', 'Min', 'Max', 'Q1', 'Q3', \n",
    "        'Lower_Bound', 'Upper_Bound', 'Num_Outliers', 'Outlier_Rate'\n",
    "    ])\n",
    "    \n",
    "    return outlier_table.sort_values('Outlier_Rate', ascending=False).reset_index(drop=True)\n",
    "\n",
    "numeric_cols = ['price', 'bed', 'bath', 'acre_lot', 'house_size']\n",
    "outlier_table = detect_outliers_iqr(df, numeric_cols)\n",
    "print(outlier_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d4063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories: [array(['100 89 Lower Shepard Creek Road', '139th Ave Unit Peck',\n",
      "       '15th Ave Milton', ..., 'Zuni', 'Zwingle', 'Zwolle'],\n",
      "      shape=(20045,), dtype=object), array(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n",
      "       'Colorado', 'Connecticut', 'Delaware', 'District of Columbia',\n",
      "       'Florida', 'Georgia', 'Guam', 'Hawaii', 'Idaho', 'Illinois',\n",
      "       'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine',\n",
      "       'Maryland', 'Massachusetts', 'Michigan', 'Minnesota',\n",
      "       'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada',\n",
      "       'New Brunswick', 'New Hampshire', 'New Jersey', 'New Mexico',\n",
      "       'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma',\n",
      "       'Oregon', 'Pennsylvania', 'Puerto Rico', 'Rhode Island',\n",
      "       'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah',\n",
      "       'Vermont', 'Virgin Islands', 'Virginia', 'Washington',\n",
      "       'West Virginia', 'Wisconsin', 'Wyoming'], dtype=object), array(['for_sale', 'ready_to_build', 'sold'], dtype=object)]\n",
      "feature names: ['city_100 89 Lower Shepard Creek Road' 'city_139th Ave Unit Peck'\n",
      " 'city_15th Ave Milton' ... 'status_for_sale' 'status_ready_to_build'\n",
      " 'status_sold']\n",
      "X_train transformed\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 6541008 stored elements and shape (2180336, 20103)>\n",
      "  Coords\tValues\n",
      "  (0, 14405)\t1.0\n",
      "  (0, 20084)\t1.0\n",
      "  (0, 20102)\t1.0\n",
      "  (1, 392)\t1.0\n",
      "  (1, 20046)\t1.0\n",
      "  (1, 20100)\t1.0\n",
      "  (2, 5314)\t1.0\n",
      "  (2, 20045)\t1.0\n",
      "  (2, 20100)\t1.0\n",
      "  (3, 4515)\t1.0\n",
      "  (3, 20096)\t1.0\n",
      "  (3, 20100)\t1.0\n",
      "  (4, 11527)\t1.0\n",
      "  (4, 20089)\t1.0\n",
      "  (4, 20100)\t1.0\n",
      "  (5, 8185)\t1.0\n",
      "  (5, 20048)\t1.0\n",
      "  (5, 20100)\t1.0\n",
      "  (6, 3624)\t1.0\n",
      "  (6, 20063)\t1.0\n",
      "  (6, 20102)\t1.0\n",
      "  (7, 10534)\t1.0\n",
      "  (7, 20059)\t1.0\n",
      "  (7, 20100)\t1.0\n",
      "  (8, 12732)\t1.0\n",
      "  :\t:\n",
      "  (2180327, 20102)\t1.0\n",
      "  (2180328, 3138)\t1.0\n",
      "  (2180328, 20060)\t1.0\n",
      "  (2180328, 20100)\t1.0\n",
      "  (2180329, 6440)\t1.0\n",
      "  (2180329, 20066)\t1.0\n",
      "  (2180329, 20100)\t1.0\n",
      "  (2180330, 2568)\t1.0\n",
      "  (2180330, 20080)\t1.0\n",
      "  (2180330, 20100)\t1.0\n",
      "  (2180331, 19038)\t1.0\n",
      "  (2180331, 20080)\t1.0\n",
      "  (2180331, 20100)\t1.0\n",
      "  (2180332, 10250)\t1.0\n",
      "  (2180332, 20049)\t1.0\n",
      "  (2180332, 20102)\t1.0\n",
      "  (2180333, 19967)\t1.0\n",
      "  (2180333, 20085)\t1.0\n",
      "  (2180333, 20100)\t1.0\n",
      "  (2180334, 1008)\t1.0\n",
      "  (2180334, 20064)\t1.0\n",
      "  (2180334, 20100)\t1.0\n",
      "  (2180335, 2911)\t1.0\n",
      "  (2180335, 20075)\t1.0\n",
      "  (2180335, 20100)\t1.0\n",
      "X_test transformed\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 66711 stored elements and shape (22249, 20103)>\n",
      "  Coords\tValues\n",
      "  (0, 7518)\t1.0\n",
      "  (0, 20085)\t1.0\n",
      "  (0, 20102)\t1.0\n",
      "  (1, 12898)\t1.0\n",
      "  (1, 20070)\t1.0\n",
      "  (1, 20100)\t1.0\n",
      "  (2, 14576)\t1.0\n",
      "  (2, 20087)\t1.0\n",
      "  (2, 20102)\t1.0\n",
      "  (3, 13413)\t1.0\n",
      "  (3, 20063)\t1.0\n",
      "  (3, 20102)\t1.0\n",
      "  (4, 13579)\t1.0\n",
      "  (4, 20065)\t1.0\n",
      "  (4, 20100)\t1.0\n",
      "  (5, 5422)\t1.0\n",
      "  (5, 20080)\t1.0\n",
      "  (5, 20102)\t1.0\n",
      "  (6, 2096)\t1.0\n",
      "  (6, 20079)\t1.0\n",
      "  (6, 20100)\t1.0\n",
      "  (7, 18863)\t1.0\n",
      "  (7, 20079)\t1.0\n",
      "  (7, 20102)\t1.0\n",
      "  (8, 19483)\t1.0\n",
      "  :\t:\n",
      "  (22240, 20100)\t1.0\n",
      "  (22241, 1866)\t1.0\n",
      "  (22241, 20067)\t1.0\n",
      "  (22241, 20100)\t1.0\n",
      "  (22242, 15703)\t1.0\n",
      "  (22242, 20054)\t1.0\n",
      "  (22242, 20100)\t1.0\n",
      "  (22243, 19562)\t1.0\n",
      "  (22243, 20085)\t1.0\n",
      "  (22243, 20100)\t1.0\n",
      "  (22244, 2309)\t1.0\n",
      "  (22244, 20063)\t1.0\n",
      "  (22244, 20100)\t1.0\n",
      "  (22245, 1203)\t1.0\n",
      "  (22245, 20048)\t1.0\n",
      "  (22245, 20102)\t1.0\n",
      "  (22246, 7230)\t1.0\n",
      "  (22246, 20055)\t1.0\n",
      "  (22246, 20102)\t1.0\n",
      "  (22247, 16924)\t1.0\n",
      "  (22247, 20096)\t1.0\n",
      "  (22247, 20100)\t1.0\n",
      "  (22248, 1133)\t1.0\n",
      "  (22248, 20049)\t1.0\n",
      "  (22248, 20100)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing Pipeline\n",
    "#-------------------------------------------\n",
    "\n",
    "\n",
    "# preprocess with pipeline and columntransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "# collect the various features\n",
    "cat_ftrs = ['status','city','state']\n",
    "num_ftrs = ['bed','bath','acre_lot','house_size']\n",
    "broker_ftr = ['brokered_by']\n",
    "street_ftr = ['street']\n",
    "zip_ftr = ['zip_code']\n",
    "sold_ftr = ['prev_sold_date']\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "class BinaryTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        # NA → 0；非 NA → 1\n",
    "        return np.where(pd.isna(X), 0, 1).reshape(-1,1)\n",
    "\n",
    "class ZipCodeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_filled = X.fillna(\"000\")\n",
    "        return X_filled.values.reshape(-1,1)\n",
    "    \n",
    "class PrevSoldDateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        dates = pd.to_datetime(X, errors='coerce')\n",
    "        years = (datetime.now() - dates).dt.days / 365\n",
    "        years = years.fillna(-1)\n",
    "        return years.values.reshape(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "# one-hot encoder and imputer\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant',fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False,handle_unknown='ignore'))])\n",
    "# standard scaler and imputer\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', IterativeImputer(max_iter=10, random_state=42)),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "broker_pipe = Pipeline(steps=[\n",
    "    ('binary', BinaryTransformer())\n",
    "])\n",
    "street_pipe = Pipeline(steps=[\n",
    "    ('binary', BinaryTransformer())\n",
    "])\n",
    "zip_pipe = Pipeline(steps=[\n",
    "    ('zip_transform', ZipCodeTransformer()),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "sold_pipe = Pipeline(steps=[\n",
    "    ('sold_transform', PrevSoldDateTransformer()),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# collect the transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_ftrs),\n",
    "        ('cat', categorical_transformer, cat_ftrs),\n",
    "        ('broker', broker_pipe, broker_ftr),\n",
    "        ('street', street_pipe, street_ftr),\n",
    "        ('zip', zip_pipe, zip_ftr),\n",
    "        ('sold', sold_pipe, sold_ftr)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a4771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical feature 后续可能需要继续处理\n",
    "\n",
    "# Handle missing values\n",
    "\n",
    "# brokered_by\n",
    "# NA means no broker → fill with 0； otherwise 1\n",
    "# 我感觉直接分成有没有agent更好，变成dummy\n",
    "X_train['brokered_by'] = X_train['brokered_by'].fillna('Unknown Broker')#需要修改\n",
    "\n",
    "# zip_code 000 for NA\n",
    "X_train['zip_code'] = X_train['zip_code'].fillna('Unknown Zip') #需要修改\n",
    "\n",
    "# street\n",
    "# NA means unknown street → fill with 0; otherwise 1\n",
    "X_train['street'] = X_train['street'].fillna('Unknown Street') #需要修改\n",
    "\n",
    "# prev_sold_date\n",
    "# NA means haven't sold → fill with -1; otherwise calculate years since last sold   \n",
    "from datetime import datetime\n",
    "# 将日期字符串转为 datetime\n",
    "X_train['prev_sold_date'] = pd.to_datetime(X_train['prev_sold_date'], errors='coerce')\n",
    "# 计算距今年份差\n",
    "X_train['years_since_last_sold'] = (datetime.now() - X_train['prev_sold_date']).dt.days / 365\n",
    "# 如果缺失，表示从未售出\n",
    "X_train['years_since_last_sold'] = X_train['years_since_last_sold'].fillna(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b821f436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.10016716e+00  3.04370068e-01 -1.93011668e-02  2.10544673e-03]\n",
      " [ 4.62158251e-01  3.04370068e-01             nan -1.33937269e-03]\n",
      " [            nan             nan  9.06133346e-04             nan]\n",
      " ...\n",
      " [ 4.62158251e-01  9.08870851e-01 -1.95090876e-02  9.83159001e-05]\n",
      " [ 4.62158251e-01  3.04370068e-01 -1.94311173e-02 -5.87464006e-04]\n",
      " [            nan             nan -1.97559935e-02             nan]]\n",
      "[[ 4.62158251e-01  3.04370068e-01 -1.92621816e-02 -6.21076137e-05]\n",
      " [-1.75850653e-01 -3.00130714e-01 -1.92361915e-02             nan]\n",
      " [ 4.62158251e-01  3.04370068e-01 -1.97429984e-02 -1.32467741e-03]\n",
      " ...\n",
      " [ 4.62158251e-01 -3.00130714e-01 -1.95480727e-02 -8.91166536e-04]\n",
      " [-1.75850653e-01 -3.00130714e-01 -1.96780232e-02 -1.57204801e-03]\n",
      " [            nan             nan  2.91995216e-01             nan]]\n"
     ]
    }
   ],
   "source": [
    "# continuous feature scaling with StandardScaler\n",
    "\n",
    "# 处理outliers需要进一步的去问，因为不知道这些值是不是错误数据\n",
    "\n",
    "X_train_countinuous = X_train[['bed', 'bath', 'acre_lot', 'house_size']]\n",
    "X_test_countinuous = X_test[['bed', 'bath', 'acre_lot', 'house_size']]\n",
    "scaler = StandardScaler()\n",
    "print(scaler.fit_transform(X_train_countinuous))\n",
    "print(scaler.transform(X_test_countinuous))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d6604279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# 将日期字符串转为 datetime\n",
    "X_train['prev_sold_date'] = pd.to_datetime(X_train['prev_sold_date'], errors='coerce')\n",
    "# 计算距今年份差\n",
    "X_train['years_since_last_sold'] = (datetime.now() - X_train['prev_sold_date']).dt.days / 365\n",
    "# 如果缺失，表示从未售出\n",
    "X_train['years_since_last_sold'] = X_train['years_since_last_sold'].fillna(-1)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
